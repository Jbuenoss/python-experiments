{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyP61kNpKHuHPmDEikaBB4+z"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OQ6ujZecoTqC","executionInfo":{"status":"ok","timestamp":1710516333264,"user_tz":180,"elapsed":1565,"user":{"displayName":"Um cara esbelta","userId":"03656793953301769847"}},"outputId":"ad4c6c15-06b3-4ba4-d4fa-da55e54bbb4e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 9912422/9912422 [00:00<00:00, 89417246.63it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28881/28881 [00:00<00:00, 57301652.71it/s]"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["\n","100%|██████████| 1648877/1648877 [00:00<00:00, 24393043.84it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4542/4542 [00:00<00:00, 9640955.85it/s]"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["import torch\n","import torchvision\n","\n","#Criando um transformador de imgs em tensores.\n","#torch só lida com tensores(forma númerica de representar informações)\n","transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n","\n","#baixar o dataset, sem download e diferença no train\n","# trainset = torchvision.datasets.LSUN(root='./data',  classes='train', transform=transform)\n","# testset = torchvision.datasets.LSUN(root='./data', classes='test', transform=transform)\n","\n","#70mil\n","trainset = torchvision.datasets.MNIST(root='./data',train=True, download=True, transform=transform)\n","testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n","\n","#divisão, serem processados juntos\n","#testar 32, 64, 128 e 256\n","batch_size = 32\n","\n","#dividir o dataset\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n","testloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)"]},{"cell_type":"code","source":["import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","#erro no calculo de entrada e saida\n","# class MNISTModel(nn.Module):\n","#       def __init__(self):\n","#         super().__init__()\n","\n","#         self.conv1 = nn.Conv2d(1, 28, kernel_size=(3,3), stride=1, padding=1)\n","#         self.act1 = nn.ReLU()\n","#         self.pool1 = nn.MaxPool2d(kernel_size=(2, 2))\n","\n","#         self.conv2 = nn.Conv2d(28, 28, kernel_size=(3,3), stride=1, padding=1)\n","#         self.act2 = nn.ReLU()\n","#         self.pool2 = nn.MaxPool2d(kernel_size=(2, 2))\n","\n","#         self.flat = nn.Flatten()\n","\n","#         self.fc3 = nn.Linear(56*28, 28)\n","#         self.act3 = nn.ReLU()\n","#         self.drop3 = nn.Dropout(0.5)\n","#         self.fc4 = nn.Linear(28, 10)\n","\n","#       def forward(self, x):\n","#         x = self.act1(self.conv1(x))\n","#         x = self.act2(self.conv2(x))\n","#         x = self.pool2(x)\n","#         x = self.flat(x)\n","#         x = self.act3(self.fc3(x))\n","#         x = self.drop3(x)\n","#         x = self.fc4(x)\n","#         return x\n","\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n","        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n","        self.conv2_drop = nn.Dropout2d()\n","        #camada conectada, camada linear\n","        self.fc1 = nn.Linear(320, 50)\n","        self.fc2 = nn.Linear(50, 10)\n","\n","    def forward(self, x):\n","        #primeira camada convolucional seguida de uma função de ativação ReLU e uma operação de max pooling\n","        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n","        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n","        #Redimensionar o tensor de saída, -1: automaticamente\n","        x = x.view(-1, 320)\n","        x = F.relu(self.fc1(x))\n","        x = F.dropout(x, training=self.training)\n","        x = self.fc2(x)\n","        #aplica a função de ativação log-softmax à saída da segunda camada totalmente conectada.\n","        #paça a chance de ser a classe\n","        return F.log_softmax(x)\n","\n","\n","\n","# model = MNISTModel()\n","model = Net()\n","loss_fn = nn.CrossEntropyLoss()\n","#lr maior: divergir\n","#colocar lr maior no começo e depois diminuir?\n","optimizer = optim.SGD(model.parameters(), lr=0.005, momentum=0.9)\n","\n","#hodout\n","n_epochs = 10\n","for epoch in range(n_epochs):\n","    print(f'epoch {epoch}')\n","    for inputs, labels in trainloader:\n","        # forward, backward, and then weight update\n","        y_pred = model(inputs)\n","        loss = loss_fn(y_pred, labels)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    acc = 0\n","    count = 0\n","    for inputs, labels in testloader:\n","        y_pred = model(inputs)\n","        acc += (torch.argmax(y_pred, 1) == labels).float().sum()\n","        count += len(labels)\n","    acc /= count\n","    print(\"Epoch %d: model accuracy %.2f%%\" % (epoch, acc*100))\n","\n","torch.save(model.state_dict(), \"MNISTmodel.pth\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C_FVLKGSuOhs","executionInfo":{"status":"ok","timestamp":1710519017279,"user_tz":180,"elapsed":523272,"user":{"displayName":"Um cara esbelta","userId":"03656793953301769847"}},"outputId":"a5bb7ddd-6409-4c4b-afc7-358c9dcf04f2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["epoch 0\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-37-18700019b494>:49: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.log_softmax(x)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0: model accuracy 90.20%\n","epoch 1\n","Epoch 1: model accuracy 92.78%\n","epoch 2\n","Epoch 2: model accuracy 94.16%\n","epoch 3\n","Epoch 3: model accuracy 94.62%\n","epoch 4\n","Epoch 4: model accuracy 95.08%\n","epoch 5\n","Epoch 5: model accuracy 95.53%\n","epoch 6\n","Epoch 6: model accuracy 95.59%\n","epoch 7\n","Epoch 7: model accuracy 95.94%\n","epoch 8\n","Epoch 8: model accuracy 95.34%\n","epoch 9\n","Epoch 9: model accuracy 96.19%\n"]}]}]}